{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MasterHive Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to develop a basis RAG system that prototypes a custom chat_with_your_doc business model.\n",
    "\n",
    "To achive this, we need to develop this in a multiphase relationship\n",
    "- Chuncking and Indexing\n",
    "- Embedding and Storage\n",
    "- Retrieval and Reranking\n",
    "- Storage and Chat History Management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langgraph langchain-google-genai langchain_community chromadb rank-bm25 transformers pypdf nest-asyncio pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.prompt_template import format_document\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"GOOGLE_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and parse the website data\n",
    "loader = WebBaseLoader(\"https://blog.google/technology/ai/google-gemini-ai/\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the text from the website data document\n",
    "text_content = docs[0].page_content\n",
    "\n",
    "# The text content between the substrings \"code, audio, image and video.\" to\n",
    "# \"Cloud TPU v5p\" is relevant for this. You can use Python's `split()`\n",
    "# to select the required content.\n",
    "text_content_1 = text_content.split(\"code, audio, image and video.\",1)[1]\n",
    "final_text = text_content_1.split(\"Cloud TPU v5p\",1)[0]\n",
    "\n",
    "# Convert the text to LangChain's `Document` format\n",
    "docs = [Document(page_content=final_text, metadata={\"source\": \"local\"})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gemini's embedding model\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk: Store the data using Chroma\n",
    "vectorstore = Chroma.from_documents(\n",
    "                     documents=docs,                 # Data\n",
    "                     embedding=gemini_embeddings,    # Embedding model\n",
    "                     persist_directory=\"./chroma_db\" # Directory to save data\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever using Chroma\n",
    "# Load from disk\n",
    "vectorstore_disk = Chroma(\n",
    "                        persist_directory=\"./chroma_db\",     \n",
    "                        embedding_function=gemini_embeddings  \n",
    "                   )\n",
    "# Get the Retriever interface for the store to use later.\n",
    "# When an unstructured query is given to a retriever it will return documents.\n",
    "#\n",
    "# Since only 1 document is stored in the Chroma vector store, search_kwargs `k`\n",
    "# is set to 1 to decrease the `k` value of chroma's similarity search from 4 to\n",
    "# 1. If you don't pass this value, you will get a warning.\n",
    "retriever = vectorstore_disk.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# Check if the retriever is working by trying to fetch the relevant docs related\n",
    "# to the word 'MMLU' (Massive Multitask Language Understanding). \n",
    "# If the length is greater than zero, it means that the retriever is functioning well.\n",
    "print(len(retriever.get_relevant_documents(\"MMLU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializa Gemini\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# To configure model parameters use the `generation_config` parameter.\n",
    "# eg. generation_config = {\"temperature\": 0.7, \"topP\": 0.8, \"topK\": 40}\n",
    "# If you only want to set a custom temperature for the model use the\n",
    "# \"temperature\" parameter directly.\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template to query Gemini\n",
    "llm_prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use five sentences maximum and keep the answer concise.\\n\n",
    "Question: {question} \\nContext: {context} \\nAnswer:\"\"\"\n",
    "\n",
    "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
    "\n",
    "print(llm_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data from documents to readable string format.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create stuff documents chain using LCEL.\n",
    "#\n",
    "# This is called a chain because you are chaining together different elements\n",
    "# with the LLM. In the following example, to create the stuff chain, you will\n",
    "# combine the relevant context from the website data matching the question, the\n",
    "# LLM model, and the output parser together like a chain using LCEL.\n",
    "#\n",
    "# The chain implements the following pipeline:\n",
    "# 1. Extract the website data relevant to the question from the Chroma\n",
    "#    vector store and save it to the variable `context`.\n",
    "# 2. `RunnablePassthrough` option to provide `question` when invoking\n",
    "#    the chain.\n",
    "# 3. The `context` and `question` are then passed to the prompt where they\n",
    "#    are populated in the respective variables.\n",
    "# 4. This prompt is then passed to the LLM (`gemini-2.0-flash`).\n",
    "# 5. Output from the LLM is passed through an output parser\n",
    "#    to structure the model's response.\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | llm_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"What is Gemini?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it all together into a simple RAG\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.prompt_template import format_document\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"API_Key\"\n",
    "\n",
    "# Read and parse the website data\n",
    "loader = WebBaseLoader(\"https://blog.google/technology/ai/google-gemini-ai/\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Extract the text from the website data document\n",
    "text_content = docs[0].page_content\n",
    "\n",
    "# The text content between the substrings \"code, audio, image and video.\" to\n",
    "# \"Cloud TPU v5p\" is relevant for this tutorial. You can use Python's `split()`\n",
    "# to select the required content.\n",
    "text_content_1 = text_content.split(\"code, audio, image and video.\",1)[1]\n",
    "final_text = text_content_1.split(\"Cloud TPU v5p\",1)[0]\n",
    "\n",
    "# Convert the text to LangChain's `Document` format\n",
    "docs = [Document(page_content=final_text, metadata={\"source\": \"local\"})]\n",
    "\n",
    "# Initialize Gemini's embedding model\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Save to disk: Store the data using Chroma\n",
    "vectorstore = Chroma.from_documents(\n",
    "                     documents=docs,                 # Data\n",
    "                     embedding=gemini_embeddings,    # Embedding model\n",
    "                     persist_directory=\"./chroma_db\" # Directory to save data\n",
    "                     )\n",
    "\n",
    "# Create a retriever using Chroma\n",
    "# Load from disk\n",
    "vectorstore_disk = Chroma(\n",
    "                        persist_directory=\"./chroma_db\",       # Directory of db\n",
    "                        embedding_function=gemini_embeddings   # Embedding model\n",
    "                   )\n",
    "\n",
    "retriever = vectorstore_disk.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# the retriever is functioning well.\n",
    "print(len(retriever.get_relevant_documents(\"MMLU\")))\n",
    "\n",
    "# Initializa Gemini\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "\n",
    "# Prompt template to query Gemini\n",
    "llm_prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use five sentences maximum and keep the answer concise.\\n\n",
    "Question: {question} \\nContext: {context} \\nAnswer:\"\"\"\n",
    "\n",
    "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
    "print(llm_prompt)\n",
    "\n",
    "\n",
    "# Combine data from documents to readable string format.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | llm_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Gemini?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Langgraph and Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import PromptTemplate, hub\n",
    "from langchain.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document, StrOutputParser\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "from typing import Dict, List, TypedDict\n",
    "import requests\n",
    "from rank_bm25 import BM25Okapi  # For hybrid retrieval\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification  # For reranking\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"GOOGLE_API_KEY\"\n",
    "\n",
    "\n",
    "# 1. Improved Document Processing --------------------------------\n",
    "def load_and_chunk_data(source: str, is_pdf: bool = False):\n",
    "    \"\"\"Load and split documents with overlap for context preservation\"\"\"\n",
    "    if is_pdf:\n",
    "        loader = PyPDFLoader(source)\n",
    "    else:\n",
    "        loader = WebBaseLoader(source)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    return loader.load_and_split(text_splitter)\n",
    "\n",
    "# Process multiple documents (PDF example)\n",
    "docs = load_and_chunk_data(\"/content/Rag_data/Talent Relocation Handbook.pdf\", is_pdf=True)\n",
    "\n",
    "# 2. Initialize Chroma Vector Store ------------------------------\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=gemini_embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "# Load the persisted vector store from disk\n",
    "vectorstore_disk = Chroma(\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=gemini_embeddings\n",
    ")\n",
    "\n",
    "# 3. Hybrid Retrieval System --------------------------------------\n",
    "class HybridRetriever:\n",
    "    def __init__(self, vector_store, text_field=\"page_content\"):\n",
    "        self.vector_store = vector_store\n",
    "\n",
    "        # Extract documents from the vector store using similarity_search\n",
    "        self.documents = vector_store.similarity_search(\"\", k=1000)  # Fetch all documents\n",
    "        print(f\"Loaded {len(self.documents)} documents from the vector store.\")\n",
    "\n",
    "        # # Extract documents from the vector store\n",
    "        # self.documents = vector_store.get()[\"documents\"]\n",
    "        # print(f\"Loaded {len(self.documents)} documents from the vector store.\")\n",
    "\n",
    "        # Initialize BM25Retriever with the documents\n",
    "        self.bm25_retriever = BM25Retriever.from_documents(\n",
    "            self.documents,\n",
    "            text_field=text_field\n",
    "        )\n",
    "        self.bm25_retriever.k = 5\n",
    "\n",
    "    def retrieve(self, query: str):\n",
    "        # Semantic Search\n",
    "        vector_results = self.vector_store.similarity_search(query, k=5)\n",
    "        # Keyword Search\n",
    "        bm25_docs = self.bm25_retriever.get_relevant_documents(query)\n",
    "        # Combine and deduplicate\n",
    "        combined = vector_results + bm25_docs\n",
    "        seen = set()\n",
    "        return [doc for doc in combined if not (doc.page_content in seen or seen.add(doc.page_content))]\n",
    "\n",
    "\n",
    "# 3. Reranking System ---------------------------------------------\n",
    "class Reranker:\n",
    "    def __init__(self, model_name=\"BAAI/bge-reranker-base\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    def rerank(self, query: str, documents: List[Document], top_n: int = 3):\n",
    "        pairs = [(query, doc.page_content) for doc in documents]\n",
    "        features = self.tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        scores = self.model(**features).logits\n",
    "        sorted_indices = scores.argsort(descending=True)\n",
    "        return [documents[i] for i in sorted_indices[:top_n]]\n",
    "\n",
    "# 4. LangGraph Workflow -------------------------------------------\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def retrieve_nodes(state: GraphState):\n",
    "    hybrid_retriever = HybridRetriever(vectorstore_disk)\n",
    "    initial_docs = hybrid_retriever.retrieve(state[\"question\"])\n",
    "    reranker = Reranker()\n",
    "    state[\"context\"] = reranker.rerank(state[\"question\"], initial_docs)\n",
    "    return state\n",
    "\n",
    "def generate_answer(state: GraphState):\n",
    "    formatted_docs = \"\\n\\n\".join([d.page_content for d in state[\"context\"]])\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "    [Advanced RAG System]\n",
    "    Context: {context}\n",
    "    ---\n",
    "    Question: {question}\n",
    "    Answer in markdown with sources. If unsure, say \"I don't know\".\n",
    "    \"\"\")\n",
    "\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.3)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    state[\"answer\"] = chain.invoke({\n",
    "        \"question\": state[\"question\"],\n",
    "        \"context\": formatted_docs\n",
    "    })\n",
    "    return state\n",
    "\n",
    "\n",
    "# 5. Full Pipeline Execution --------------------------------------\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"retrieve\", retrieve_nodes)\n",
    "workflow.add_node(\"generate\", generate_answer)\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "\n",
    "# Run the workflow\n",
    "app = workflow.compile()\n",
    "result = app.invoke({\"question\": \"What do I stand to gain with this new company as a talent?\"})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging and a mix of Keyword and semantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s refine and optimize the code further to ensure it’s production-ready and scalable.\n",
    "\n",
    "Key Improvements to the Code\n",
    "1. **Error Handling and Logging:**\n",
    "  - Add robust error handling for API calls, document loading, and retrieval.\n",
    "  - Implement logging for debugging and monitoring.\n",
    "\n",
    "2. **Optimized Hybrid Retrieval:**\n",
    "\n",
    "  - Ensure the BM25 retriever is initialized correctly with all documents.\n",
    "\n",
    "  - Improve deduplication logic for combined results.\n",
    "\n",
    "3. **Reranking Efficiency:**\n",
    "\n",
    "  - Use a lightweight reranking model for faster inference.\n",
    "\n",
    "  - Add batching for reranking to handle large document sets.\n",
    "\n",
    "4. **LangGraph Workflow Enhancements:**\n",
    "\n",
    "  - Add a fallback mechanism for failed retrievals.\n",
    "\n",
    "  - Include query rewriting or expansion for better retrieval.\n",
    "\n",
    "5. **Production Readiness:**\n",
    "\n",
    "  - Add configuration management (e.g., using pydantic or environment variables).\n",
    "\n",
    "  - Containerize the application for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import List, Dict, TypedDict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document, StrOutputParser\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set API key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"GEMINI_API_KEY\"\n",
    "\n",
    "# 1. Improved Document Processing for Multiple PDFs --------------------------------\n",
    "def load_and_chunk_pdf(file_path: str) -> List[Document]:\n",
    "    \"\"\"Load and split a single PDF document.\"\"\"\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        return loader.load_and_split(text_splitter)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_and_chunk_folder(folder_path: str) -> List[Document]:\n",
    "    \"\"\"Load and chunk all PDFs in a folder.\"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise ValueError(f\"{folder_path} is not a valid directory.\")\n",
    "\n",
    "    pdf_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".pdf\")]\n",
    "    if not pdf_files:\n",
    "        logger.warning(f\"No PDF files found in {folder_path}.\")\n",
    "        return []\n",
    "\n",
    "    all_docs = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(load_and_chunk_pdf, pdf_file) for pdf_file in pdf_files]\n",
    "        for future in futures:\n",
    "            try:\n",
    "                docs = future.result()\n",
    "                all_docs.extend(docs)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing PDF: {e}\")\n",
    "\n",
    "    logger.info(f\"Loaded and chunked {len(all_docs)} documents from {len(pdf_files)} PDFs.\")\n",
    "    return all_docs\n",
    "\n",
    "# Process all PDFs in a folder\n",
    "try:\n",
    "    folder_path = \"/content/Rag_data\"  # Replace with your folder path\n",
    "    docs = load_and_chunk_folder(folder_path)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to process folder: {e}\")\n",
    "    raise\n",
    "\n",
    "# 2. Initialize Chroma Vector Store ------------------------------\n",
    "try:\n",
    "    gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=gemini_embeddings,\n",
    "        persist_directory=\"./chroma_db\"\n",
    "    )\n",
    "    logger.info(\"Vector store initialized and persisted.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error initializing vector store: {e}\")\n",
    "    raise\n",
    "\n",
    "# Load the persisted vector store from disk\n",
    "try:\n",
    "    vectorstore_disk = Chroma(\n",
    "        persist_directory=\"./chroma_db\",\n",
    "        embedding_function=gemini_embeddings\n",
    "    )\n",
    "    logger.info(\"Loaded vector store from disk.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading vector store from disk: {e}\")\n",
    "    raise\n",
    "\n",
    "# 3. Advanced Indexing: Hierarchical Indexing --------------------------------\n",
    "class HierarchicalIndex:\n",
    "    def __init__(self, documents: List[Document], embedding_model):\n",
    "        self.documents = documents\n",
    "        self.embedding_model = embedding_model\n",
    "        self.index = self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        \"\"\"Build a hierarchical index using embeddings.\"\"\"\n",
    "        embeddings = self.embedding_model.embed_documents([doc.page_content for doc in self.documents])\n",
    "        self.index = self._cluster_documents(embeddings)\n",
    "        return self.index\n",
    "\n",
    "    def _cluster_documents(self, embeddings, n_clusters=10):\n",
    "        \"\"\"Cluster documents into hierarchical groups.\"\"\"\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        clusters = kmeans.fit_predict(embeddings)\n",
    "        return {i: [] for i in range(n_clusters)}\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5) -> List[Document]:\n",
    "        \"\"\"Retrieve documents from the hierarchical index.\"\"\"\n",
    "        query_embedding = self.embedding_model.embed_query(query)\n",
    "        # For simplicity, return top-k documents (can be enhanced with hierarchical search)\n",
    "        return self.documents[:top_k]\n",
    "\n",
    "# 4. Contextual Embeddings --------------------------------\n",
    "class ContextualEmbeddings:\n",
    "    def __init__(self, model_name=\"models/embedding-001\"):\n",
    "        self.model = GoogleGenerativeAIEmbeddings(model=model_name)\n",
    "\n",
    "    def embed(self, text: str) -> List[float]:\n",
    "        return self.model.embed_query(text)\n",
    "\n",
    "    def embed_documents(self, documents: List[str]) -> List[List[float]]:\n",
    "        return self.model.embed_documents(documents)\n",
    "\n",
    "# 5. Hybrid Retrieval System --------------------------------\n",
    "class HybridRetriever:\n",
    "    def __init__(self, vector_store, bm25_retriever):\n",
    "        self.vector_store = vector_store\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5) -> List[Document]:\n",
    "        # Semantic Search\n",
    "        vector_results = self.vector_store.similarity_search(query, k=top_k)\n",
    "        # Keyword Search\n",
    "        bm25_docs = self.bm25_retriever.get_relevant_documents(query)\n",
    "        # Combine and deduplicate\n",
    "        combined = vector_results + bm25_docs\n",
    "        seen = set()\n",
    "        return [doc for doc in combined if not (doc.page_content in seen or seen.add(doc.page_content))]\n",
    "\n",
    "# 6. Reranking System --------------------------------\n",
    "class Reranker:\n",
    "    def __init__(self, model_name=\"BAAI/bge-reranker-base\"):\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "            logger.info(f\"Loaded reranking model: {model_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading reranking model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def rerank(self, query: str, documents: List[Document], top_n: int = 3) -> List[Document]:\n",
    "        try:\n",
    "            pairs = [(query, doc.page_content) for doc in documents]\n",
    "            features = self.tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            scores = self.model(**features).logits\n",
    "            sorted_indices = scores.argsort(descending=True)\n",
    "            return [documents[i] for i in sorted_indices[:top_n]]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during reranking: {e}\")\n",
    "            return documents[:top_n]  # Fallback to top N documents\n",
    "\n",
    "# 7. LangGraph Workflow --------------------------------\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve_nodes(state: GraphState) -> GraphState:\n",
    "    try:\n",
    "        # Initialize components\n",
    "        embeddings = ContextualEmbeddings()\n",
    "        hierarchical_index = HierarchicalIndex(docs, embeddings)\n",
    "        bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "        hybrid_retriever = HybridRetriever(vectorstore_disk, bm25_retriever)\n",
    "        reranker = Reranker()\n",
    "\n",
    "        # Retrieve documents\n",
    "        initial_docs = hybrid_retriever.retrieve(state[\"question\"])\n",
    "        state[\"context\"] = reranker.rerank(state[\"question\"], initial_docs)\n",
    "        logger.info(f\"Retrieved and reranked {len(state['context'])} documents.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in retrieve_nodes: {e}\")\n",
    "        state[\"context\"] = []  # Fallback to empty context\n",
    "    return state\n",
    "\n",
    "def generate_answer(state: GraphState) -> GraphState:\n",
    "    try:\n",
    "        formatted_docs = \"\\n\\n\".join([d.page_content for d in state[\"context\"]])\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        [Advanced RAG System]\n",
    "        Context: {context}\n",
    "        ---\n",
    "        Question: {question}\n",
    "        Answer in markdown with sources. If unsure, say \"I don't know\".\n",
    "        \"\"\")\n",
    "        llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.3)\n",
    "        chain = prompt | llm | StrOutputParser()\n",
    "        state[\"answer\"] = chain.invoke({\n",
    "            \"question\": state[\"question\"],\n",
    "            \"context\": formatted_docs\n",
    "        })\n",
    "        logger.info(\"Generated answer successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in generate_answer: {e}\")\n",
    "        state[\"answer\"] = \"I'm sorry, I couldn't generate an answer. Please try again.\"\n",
    "    return state\n",
    "\n",
    "# 8. Full Pipeline Execution --------------------------------\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"retrieve\", retrieve_nodes)\n",
    "workflow.add_node(\"generate\", generate_answer)\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "app = workflow.compile()\n",
    "\n",
    "# Example Query\n",
    "try:\n",
    "    result = app.invoke({\"question\": \"Run a swot analysis and tell me if this is good for me to leave my job to sign up as a professional with this new company\"})\n",
    "    print(result[\"answer\"])\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error executing workflow: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = app.invoke({\"question\": \"Using a swot analysis tell me about all the reasons not to consider this offer\"})\n",
    "    print(result[\"answer\"])\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error executing workflow: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a streaming output and a validation pipeline using pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import List, Dict, TypedDict, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document, StrOutputParser\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from pydantic import BaseModel, ValidationError\n",
    "import asyncio\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set API key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyC-zFkoipLIFe-2u4ZlamzwT-wZkHzJx-U\"\n",
    "\n",
    "# 1. Pydantic Models for Data Validation --------------------------------\n",
    "class QueryInput(BaseModel):\n",
    "    question: str\n",
    "    top_k: Optional[int] = 5\n",
    "\n",
    "class QueryOutput(BaseModel):\n",
    "    answer: str\n",
    "    sources: List[str]\n",
    "\n",
    "# 2. Improved Document Processing for Multiple PDFs --------------------------------\n",
    "def load_and_chunk_pdf(file_path: str) -> List[Document]:\n",
    "    \"\"\"Load and split a single PDF document.\"\"\"\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        return loader.load_and_split(text_splitter)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_and_chunk_folder(folder_path: str) -> List[Document]:\n",
    "    \"\"\"Load and chunk all PDFs in a folder.\"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise ValueError(f\"{folder_path} is not a valid directory.\")\n",
    "\n",
    "    pdf_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".pdf\")]\n",
    "    if not pdf_files:\n",
    "        logger.warning(f\"No PDF files found in {folder_path}.\")\n",
    "        return []\n",
    "\n",
    "    all_docs = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(load_and_chunk_pdf, pdf_file) for pdf_file in pdf_files]\n",
    "        for future in futures:\n",
    "            try:\n",
    "                docs = future.result()\n",
    "                all_docs.extend(docs)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing PDF: {e}\")\n",
    "\n",
    "    logger.info(f\"Loaded and chunked {len(all_docs)} documents from {len(pdf_files)} PDFs.\")\n",
    "    return all_docs\n",
    "\n",
    "# Process all PDFs in a folder\n",
    "try:\n",
    "    folder_path = \"/content/Rag_data\"  # Replace with your folder path\n",
    "    docs = load_and_chunk_folder(folder_path)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to process folder: {e}\")\n",
    "    raise\n",
    "\n",
    "# 3. Initialize Chroma Vector Store ------------------------------\n",
    "try:\n",
    "    gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=gemini_embeddings,\n",
    "        persist_directory=\"./chroma_db\"\n",
    "    )\n",
    "    logger.info(\"Vector store initialized and persisted.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error initializing vector store: {e}\")\n",
    "    raise\n",
    "\n",
    "# Load the persisted vector store from disk\n",
    "try:\n",
    "    vectorstore_disk = Chroma(\n",
    "        persist_directory=\"./chroma_db\",\n",
    "        embedding_function=gemini_embeddings\n",
    "    )\n",
    "    logger.info(\"Loaded vector store from disk.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading vector store from disk: {e}\")\n",
    "    raise\n",
    "\n",
    "# 4. Advanced Indexing: Hierarchical Indexing --------------------------------\n",
    "class HierarchicalIndex:\n",
    "    def __init__(self, documents: List[Document], embedding_model):\n",
    "        self.documents = documents\n",
    "        self.embedding_model = embedding_model\n",
    "        self.index = self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        \"\"\"Build a hierarchical index using embeddings.\"\"\"\n",
    "        embeddings = self.embedding_model.embed_documents([doc.page_content for doc in self.documents])\n",
    "        self.index = self._cluster_documents(embeddings)\n",
    "        return self.index\n",
    "\n",
    "    def _cluster_documents(self, embeddings, n_clusters=10):\n",
    "        \"\"\"Cluster documents into hierarchical groups.\"\"\"\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        clusters = kmeans.fit_predict(embeddings)\n",
    "        return {i: [] for i in range(n_clusters)}\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5) -> List[Document]:\n",
    "        \"\"\"Retrieve documents from the hierarchical index.\"\"\"\n",
    "        query_embedding = self.embedding_model.embed_query(query)\n",
    "        # For simplicity, return top-k documents (can be enhanced with hierarchical search)\n",
    "        return self.documents[:top_k]\n",
    "\n",
    "# 5. Contextual Embeddings --------------------------------\n",
    "class ContextualEmbeddings:\n",
    "    def __init__(self, model_name=\"models/embedding-001\"):\n",
    "        self.model = GoogleGenerativeAIEmbeddings(model=model_name)\n",
    "\n",
    "    def embed(self, text: str) -> List[float]:\n",
    "        return self.model.embed_query(text)\n",
    "\n",
    "    def embed_documents(self, documents: List[str]) -> List[List[float]]:\n",
    "        return self.model.embed_documents(documents)\n",
    "\n",
    "# 6. Hybrid Retrieval System --------------------------------\n",
    "class HybridRetriever:\n",
    "    def __init__(self, vector_store, bm25_retriever):\n",
    "        self.vector_store = vector_store\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5) -> List[Document]:\n",
    "        # Semantic Search\n",
    "        vector_results = self.vector_store.similarity_search(query, k=top_k)\n",
    "        # Keyword Search\n",
    "        bm25_docs = self.bm25_retriever.get_relevant_documents(query)\n",
    "        # Combine and deduplicate\n",
    "        combined = vector_results + bm25_docs\n",
    "        seen = set()\n",
    "        return [doc for doc in combined if not (doc.page_content in seen or seen.add(doc.page_content))]\n",
    "\n",
    "# 7. Reranking System --------------------------------\n",
    "class Reranker:\n",
    "    def __init__(self, model_name=\"BAAI/bge-reranker-base\"):\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "            logger.info(f\"Loaded reranking model: {model_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading reranking model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def rerank(self, query: str, documents: List[Document], top_n: int = 3) -> List[Document]:\n",
    "        try:\n",
    "            pairs = [(query, doc.page_content) for doc in documents]\n",
    "            features = self.tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            scores = self.model(**features).logits\n",
    "            sorted_indices = scores.argsort(descending=True)\n",
    "            return [documents[i] for i in sorted_indices[:top_n]]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during reranking: {e}\")\n",
    "            return documents[:top_n]  # Fallback to top N documents\n",
    "\n",
    "# 8. LangGraph Workflow --------------------------------\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve_nodes(state: GraphState) -> GraphState:\n",
    "    try:\n",
    "        # Initialize components\n",
    "        embeddings = ContextualEmbeddings()\n",
    "        hierarchical_index = HierarchicalIndex(docs, embeddings)\n",
    "        bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "        hybrid_retriever = HybridRetriever(vectorstore_disk, bm25_retriever)\n",
    "        reranker = Reranker()\n",
    "\n",
    "        # Retrieve documents\n",
    "        initial_docs = hybrid_retriever.retrieve(state[\"question\"])\n",
    "        state[\"context\"] = reranker.rerank(state[\"question\"], initial_docs)\n",
    "        logger.info(f\"Retrieved and reranked {len(state['context'])} documents.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in retrieve_nodes: {e}\")\n",
    "        state[\"context\"] = []  # Fallback to empty context\n",
    "    return state\n",
    "\n",
    "async def generate_answer(state: GraphState):\n",
    "    \"\"\"Stream the answer in real-time.\"\"\"\n",
    "    try:\n",
    "        formatted_docs = \"\\n\\n\".join([d.page_content for d in state[\"context\"]])\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        [Advanced RAG System]\n",
    "        Context: {context}\n",
    "        ---\n",
    "        Question: {question}\n",
    "        Answer in markdown with sources. If unsure, say \"I don't know\".\n",
    "        \"\"\")\n",
    "        llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.3)\n",
    "        chain = prompt | llm | StrOutputParser()\n",
    "        response = chain.stream({\n",
    "            \"question\": state[\"question\"],\n",
    "            \"context\": formatted_docs\n",
    "        })\n",
    "        for chunk in response:\n",
    "            yield chunk\n",
    "        logger.info(\"Generated answer successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in generate_answer: {e}\")\n",
    "        yield \"I'm sorry, I couldn't generate an answer. Please try again.\"\n",
    "\n",
    "# 9. Main Function with Pydantic Validation and Streaming --------------------------------\n",
    "async def main(question: str, top_k: int = 5):\n",
    "    # Validate input\n",
    "    try:\n",
    "        query_input = QueryInput(question=question, top_k=top_k)\n",
    "    except ValidationError as e:\n",
    "        logger.error(f\"Validation error: {e}\")\n",
    "        return\n",
    "\n",
    "    # Initialize workflow\n",
    "    workflow = StateGraph(GraphState)\n",
    "    workflow.add_node(\"retrieve\", retrieve_nodes)\n",
    "    workflow.set_entry_point(\"retrieve\")\n",
    "    workflow.add_edge(\"retrieve\", END)\n",
    "    app = workflow.compile()\n",
    "\n",
    "\n",
    "\n",
    "# Example Query\n",
    "try:\n",
    "    result = app.invoke({\"question\": \"Run a swot analysis and tell me if this is good for me to leave my job to sign up as a professional with the company\"})\n",
    "    print(result[\"answer\"])\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error executing workflow: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating advanced prompting and Chat History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import List, Dict, TypedDict, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document, StrOutputParser\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from pydantic import BaseModel, ValidationError\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Apply nest_asyncio for Jupyter/Colab compatibility\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set API key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"GOOGLE_API_KEY\"\n",
    "\n",
    "# Configuration Management\n",
    "class Config:\n",
    "    PDF_FOLDER_PATH = \"/content/Rag_data\"  # Folder containing PDFs\n",
    "    CHROMA_PERSIST_DIR = \"./chroma_db\"  # Directory to persist Chroma vector store\n",
    "    EMBEDDING_MODEL = \"models/embedding-001\"  # Gemini embedding model\n",
    "    RERANKER_MODEL = \"BAAI/bge-reranker-base\"  # Reranking model\n",
    "    GENERATION_MODEL = \"gemini-2.0-flash\"  # Gemini generation model\n",
    "    TOP_K = 5  # Default number of documents to retrieve\n",
    "    TOP_N = 3  # Default number of documents to rerank\n",
    "\n",
    "# 1. Pydantic Models for Data Validation --------------------------------\n",
    "class QueryInput(BaseModel):\n",
    "    question: str\n",
    "    top_k: Optional[int] = Config.TOP_K\n",
    "\n",
    "class QueryOutput(BaseModel):\n",
    "    answer: str\n",
    "    sources: List[str]\n",
    "\n",
    "# 2. Improved Document Processing for Multiple PDFs --------------------------------\n",
    "def load_and_chunk_pdf(file_path: str) -> List[Document]:\n",
    "    \"\"\"Load and split a single PDF document.\"\"\"\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        return loader.load_and_split(text_splitter)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_and_chunk_folder(folder_path: str) -> List[Document]:\n",
    "    \"\"\"Load and chunk all PDFs in a folder.\"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise ValueError(f\"{folder_path} is not a valid directory.\")\n",
    "\n",
    "    pdf_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".pdf\")]\n",
    "    if not pdf_files:\n",
    "        logger.warning(f\"No PDF files found in {folder_path}.\")\n",
    "        return []\n",
    "\n",
    "    all_docs = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(load_and_chunk_pdf, pdf_file) for pdf_file in pdf_files]\n",
    "        for future in futures:\n",
    "            try:\n",
    "                docs = future.result()\n",
    "                all_docs.extend(docs)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing PDF: {e}\")\n",
    "\n",
    "    logger.info(f\"Loaded and chunked {len(all_docs)} documents from {len(pdf_files)} PDFs.\")\n",
    "    return all_docs\n",
    "\n",
    "# Process all PDFs in a folder\n",
    "try:\n",
    "    docs = load_and_chunk_folder(Config.PDF_FOLDER_PATH)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to process folder: {e}\")\n",
    "    raise\n",
    "\n",
    "# 3. Initialize Chroma Vector Store ------------------------------\n",
    "try:\n",
    "    gemini_embeddings = GoogleGenerativeAIEmbeddings(model=Config.EMBEDDING_MODEL)\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=gemini_embeddings,\n",
    "        persist_directory=Config.CHROMA_PERSIST_DIR\n",
    "    )\n",
    "    logger.info(\"Vector store initialized and persisted.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error initializing vector store: {e}\")\n",
    "    raise\n",
    "\n",
    "# Load the persisted vector store from disk\n",
    "try:\n",
    "    vectorstore_disk = Chroma(\n",
    "        persist_directory=Config.CHROMA_PERSIST_DIR,\n",
    "        embedding_function=gemini_embeddings\n",
    "    )\n",
    "    logger.info(\"Loaded vector store from disk.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading vector store from disk: {e}\")\n",
    "    raise\n",
    "\n",
    "# 4. Advanced Indexing: Hierarchical Indexing --------------------------------\n",
    "class HierarchicalIndex:\n",
    "    def __init__(self, documents: List[Document], embedding_model):\n",
    "        self.documents = documents\n",
    "        self.embedding_model = embedding_model\n",
    "        self.index = self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        \"\"\"Build a hierarchical index using embeddings.\"\"\"\n",
    "        embeddings = self.embedding_model.embed_documents([doc.page_content for doc in self.documents])\n",
    "        self.index = self._cluster_documents(embeddings)\n",
    "        return self.index\n",
    "\n",
    "    def _cluster_documents(self, embeddings, n_clusters=10):\n",
    "        \"\"\"Cluster documents into hierarchical groups.\"\"\"\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        clusters = kmeans.fit_predict(embeddings)\n",
    "        return {i: [] for i in range(n_clusters)}\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = Config.TOP_K) -> List[Document]:\n",
    "        \"\"\"Retrieve documents from the hierarchical index.\"\"\"\n",
    "        query_embedding = self.embedding_model.embed_query(query)\n",
    "        # For simplicity, return top-k documents (can be enhanced with hierarchical search)\n",
    "        return self.documents[:top_k]\n",
    "\n",
    "# 5. Contextual Embeddings --------------------------------\n",
    "class ContextualEmbeddings:\n",
    "    def __init__(self, model_name=Config.EMBEDDING_MODEL):\n",
    "        self.model = GoogleGenerativeAIEmbeddings(model=model_name)\n",
    "\n",
    "    def embed(self, text: str) -> List[float]:\n",
    "        return self.model.embed_query(text)\n",
    "\n",
    "    def embed_documents(self, documents: List[str]) -> List[List[float]]:\n",
    "        return self.model.embed_documents(documents)\n",
    "\n",
    "# 6. Hybrid Retrieval System --------------------------------\n",
    "class HybridRetriever:\n",
    "    def __init__(self, vector_store, bm25_retriever):\n",
    "        self.vector_store = vector_store\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = Config.TOP_K) -> List[Document]:\n",
    "        # Semantic Search\n",
    "        vector_results = self.vector_store.similarity_search(query, k=top_k)\n",
    "        # Keyword Search\n",
    "        bm25_docs = self.bm25_retriever.get_relevant_documents(query)\n",
    "        # Combine and deduplicate\n",
    "        combined = vector_results + bm25_docs\n",
    "        seen = set()\n",
    "        return [doc for doc in combined if not (doc.page_content in seen or seen.add(doc.page_content))]\n",
    "\n",
    "# 7. Reranking System --------------------------------\n",
    "class Reranker:\n",
    "    def __init__(self, model_name=Config.RERANKER_MODEL):\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "            logger.info(f\"Loaded reranking model: {model_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading reranking model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def rerank(self, query: str, documents: List[Document], top_n: int = Config.TOP_N) -> List[Document]:\n",
    "        try:\n",
    "            pairs = [(query, doc.page_content) for doc in documents]\n",
    "            features = self.tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            scores = self.model(**features).logits\n",
    "            sorted_indices = scores.argsort(descending=True)\n",
    "            return [documents[i] for i in sorted_indices[:top_n]]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during reranking: {e}\")\n",
    "            return documents[:top_n]  # Fallback to top N documents\n",
    "\n",
    "# 8. Advanced Prompting Strategies --------------------------------\n",
    "class AdvancedPrompts:\n",
    "    @staticmethod\n",
    "    def retrieval_prompt(query: str) -> str:\n",
    "        \"\"\"Advanced prompt for retrieval.\"\"\"\n",
    "        return f\"\"\"\n",
    "        You are an expert retrieval system. Rewrite the following query to improve retrieval:\n",
    "        Original Query: {query}\n",
    "        Rewritten Query:\n",
    "        \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def metadata_filter_prompt(query: str, metadata: Dict[str, str]) -> str:\n",
    "        \"\"\"Advanced prompt for metadata filtering.\"\"\"\n",
    "        return f\"\"\"\n",
    "        You are an expert retrieval system. Add metadata to the query for better filtering:\n",
    "        Original Query: {query}\n",
    "        Metadata: {metadata}\n",
    "        Enhanced Query:\n",
    "        \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def generation_prompt(context: str, question: str) -> str:\n",
    "        \"\"\"Advanced prompt for generation.\"\"\"\n",
    "        return f\"\"\"\n",
    "        You are an expert in talent management and career development. Use the following context to answer the question.\n",
    "        Context: {context}\n",
    "        ---\n",
    "        Question: {question}\n",
    "        Answer in markdown with sources. If unsure, say \"I don't know\".\n",
    "        \"\"\"\n",
    "\n",
    "# 9. Metadata Extraction and Filtering --------------------------------\n",
    "class MetadataExtractor:\n",
    "    @staticmethod\n",
    "    def extract_from_query(query: str) -> Dict[str, str]:\n",
    "        \"\"\"Extract metadata from the query using an LLM.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Extract metadata from the following query. Return the result as a JSON object with keys like \"year\", \"topic\", \"location\", etc.\n",
    "        Query: {query}\n",
    "        Metadata:\n",
    "        \"\"\"\n",
    "        llm = ChatGoogleGenerativeAI(model=Config.GENERATION_MODEL, temperature=0.3)\n",
    "        response = llm.invoke(prompt).content\n",
    "        try:\n",
    "            metadata = json.loads(response)\n",
    "            return metadata\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing metadata: {e}\")\n",
    "            return {}\n",
    "\n",
    "class MetadataInferrer:\n",
    "    @staticmethod\n",
    "    def infer_from_documents(documents: List[Document]) -> Dict[str, str]:\n",
    "        \"\"\"Infer metadata from the document set.\"\"\"\n",
    "        metadata = {}\n",
    "\n",
    "        # Example: Infer the most recent year\n",
    "        years = [doc.metadata.get(\"year\") for doc in documents if \"year\" in doc.metadata]\n",
    "        if years:\n",
    "            metadata[\"year\"] = max(years)\n",
    "\n",
    "        # Example: Infer the most common topic\n",
    "        topics = [doc.metadata.get(\"topic\") for doc in documents if \"topic\" in doc.metadata]\n",
    "        if topics:\n",
    "            metadata[\"topic\"] = Counter(topics).most_common(1)[0][0]\n",
    "\n",
    "        return metadata\n",
    "\n",
    "def combine_metadata(query_metadata: Dict[str, str], document_metadata: Dict[str, str]) -> Dict[str, str]:\n",
    "    \"\"\"Combine metadata from the query and documents.\"\"\"\n",
    "    combined = {}\n",
    "    combined.update(document_metadata)  # Start with document metadata\n",
    "    combined.update(query_metadata)    # Override with query metadata (if any)\n",
    "    return combined\n",
    "\n",
    "# 10. LangGraph Workflow --------------------------------\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve_nodes(state: GraphState) -> GraphState:\n",
    "    try:\n",
    "        # Initialize components\n",
    "        embeddings = ContextualEmbeddings()\n",
    "        hierarchical_index = HierarchicalIndex(docs, embeddings)\n",
    "        bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "        hybrid_retriever = HybridRetriever(vectorstore_disk, bm25_retriever)\n",
    "        reranker = Reranker()\n",
    "\n",
    "        # Extract metadata from the query\n",
    "        query_metadata = MetadataExtractor.extract_from_query(state[\"question\"])\n",
    "\n",
    "        # Infer metadata from the document set\n",
    "        document_metadata = MetadataInferrer.infer_from_documents(docs)\n",
    "\n",
    "        # Combine metadata\n",
    "        metadata = combine_metadata(query_metadata, document_metadata)\n",
    "\n",
    "        # Rewrite query with metadata\n",
    "        llm = ChatGoogleGenerativeAI(model=Config.GENERATION_MODEL, temperature=0.3)\n",
    "        rewritten_query = llm.invoke(AdvancedPrompts.retrieval_prompt(state[\"question\"])).content\n",
    "\n",
    "        # Add metadata filtering\n",
    "        enhanced_query = llm.invoke(AdvancedPrompts.metadata_filter_prompt(rewritten_query, metadata)).content\n",
    "\n",
    "        # Retrieve documents\n",
    "        initial_docs = hybrid_retriever.retrieve(enhanced_query)\n",
    "        state[\"context\"] = reranker.rerank(enhanced_query, initial_docs)\n",
    "        logger.info(f\"Retrieved and reranked {len(state['context'])} documents.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in retrieve_nodes: {e}\")\n",
    "        state[\"context\"] = []  # Fallback to empty context\n",
    "    return state\n",
    "\n",
    "async def generate_answer(state: GraphState):\n",
    "    \"\"\"Stream the answer in real-time.\"\"\"\n",
    "    try:\n",
    "        formatted_docs = \"\\n\\n\".join([d.page_content for d in state[\"context\"]])\n",
    "        prompt = AdvancedPrompts.generation_prompt(formatted_docs, state[\"question\"])\n",
    "        llm = ChatGoogleGenerativeAI(model=Config.GENERATION_MODEL, temperature=0.3)\n",
    "        chain = PromptTemplate.from_template(prompt) | llm | StrOutputParser()\n",
    "        response = chain.stream({\n",
    "            \"question\": state[\"question\"],\n",
    "            \"context\": formatted_docs\n",
    "        })\n",
    "        for chunk in response:\n",
    "            yield chunk\n",
    "        logger.info(\"Generated answer successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in generate_answer: {e}\")\n",
    "        yield \"I'm sorry, I couldn't generate an answer. Please try again.\"\n",
    "\n",
    "# 11. Main Function with Advanced Prompting --------------------------------\n",
    "async def main(question: str, top_k: int = Config.TOP_K):\n",
    "    # Validate input\n",
    "    try:\n",
    "        query_input = QueryInput(question=question, top_k=top_k)\n",
    "    except ValidationError as e:\n",
    "        logger.error(f\"Validation error: {e}\")\n",
    "        return\n",
    "\n",
    "    # Initialize workflow\n",
    "    workflow = StateGraph(GraphState)\n",
    "    workflow.add_node(\"retrieve\", retrieve_nodes)\n",
    "    workflow.set_entry_point(\"retrieve\")\n",
    "    workflow.add_edge(\"retrieve\", END)\n",
    "    app = workflow.compile()\n",
    "\n",
    "    # Run retrieval\n",
    "    state = app.invoke({\"question\": query_input.question})\n",
    "\n",
    "    # Stream the answer\n",
    "    print(\"Generating answer...\")\n",
    "    async for chunk in generate_answer(state):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print(\"\\nAnswer generation complete.\")\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if an event loop is already running\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "\n",
    "    # Run the main function\n",
    "    loop.run_until_complete(main(\"Run a SWOT analysis and tell me if this is good for me to leave my job to sign up as a professional with the company\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat History with MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import List, Dict, TypedDict, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document, StrOutputParser\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from pydantic import BaseModel, ValidationError\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import json\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "# Apply nest_asyncio for Jupyter/Colab compatibility\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set API key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"GOOGLE_API_KEY\"\n",
    "uri = \"mongo_db_url\"\n",
    "\n",
    "\n",
    "# Configuration Management\n",
    "class Config:\n",
    "    PDF_FOLDER_PATH = \"/content/Rag_data\"  # Folder containing PDFs\n",
    "    CHROMA_PERSIST_DIR = \"./chroma_db\"  # Directory to persist Chroma vector store\n",
    "    EMBEDDING_MODEL = \"models/embedding-001\"  # Gemini embedding model\n",
    "    RERANKER_MODEL = \"BAAI/bge-reranker-base\"  # Reranking model\n",
    "    GENERATION_MODEL = \"gemini-2.0-flash\"  # Gemini generation model\n",
    "    TOP_K = 5  # Default number of documents to retrieve\n",
    "    TOP_N = 3  # Default number of documents to rerank\n",
    "\n",
    "# Initialize MongoDB client\n",
    "# uri = \"mongo_client_uri\"\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "db = client[\"chat_history_db\"]\n",
    "chats_collection = db[\"chats\"]\n",
    "\n",
    "def store_chat_history(userID: str, chatID: str, role: str, content: str):\n",
    "    \"\"\"Store a message in the chat history.\"\"\"\n",
    "    try:\n",
    "        message = {\n",
    "            \"userID\": userID,\n",
    "            \"chatID\": chatID,\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"role\": role,\n",
    "            \"content\": content\n",
    "        }\n",
    "        chats_collection.insert_one(message)\n",
    "        logger.info(f\"Stored message in chat history for user {userID}, chat {chatID}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error storing chat history: {e}\")\n",
    "\n",
    "def retrieve_chat_history(userID: str, chatID: str, limit: int = 5) -> List[Dict[str, str]]:\n",
    "    \"\"\"Retrieve the most recent messages from the chat history.\"\"\"\n",
    "    try:\n",
    "        messages = chats_collection.find(\n",
    "            {\"userID\": userID, \"chatID\": chatID},\n",
    "            sort=[(\"timestamp\", -1)],  # Sort by timestamp in descending order\n",
    "            limit=limit\n",
    "        )\n",
    "        return [{\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in messages]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving chat history: {e}\")\n",
    "        return []\n",
    "\n",
    "# Pydantic Models for Data Validation\n",
    "class QueryInput(BaseModel):\n",
    "    question: str\n",
    "    top_k: Optional[int] = Config.TOP_K\n",
    "\n",
    "class QueryOutput(BaseModel):\n",
    "    answer: str\n",
    "    sources: List[str]\n",
    "\n",
    "# Improved Document Processing for Multiple PDFs\n",
    "def load_and_chunk_pdf(file_path: str) -> List[Document]:\n",
    "    \"\"\"Load and split a single PDF document.\"\"\"\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        return loader.load_and_split(text_splitter)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_and_chunk_folder(folder_path: str) -> List[Document]:\n",
    "    \"\"\"Load and chunk all PDFs in a folder.\"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise ValueError(f\"{folder_path} is not a valid directory.\")\n",
    "\n",
    "    pdf_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".pdf\")]\n",
    "    if not pdf_files:\n",
    "        logger.warning(f\"No PDF files found in {folder_path}.\")\n",
    "        return []\n",
    "\n",
    "    all_docs = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(load_and_chunk_pdf, pdf_file) for pdf_file in pdf_files]\n",
    "        for future in futures:\n",
    "            try:\n",
    "                docs = future.result()\n",
    "                all_docs.extend(docs)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing PDF: {e}\")\n",
    "\n",
    "    logger.info(f\"Loaded and chunked {len(all_docs)} documents from {len(pdf_files)} PDFs.\")\n",
    "    return all_docs\n",
    "\n",
    "# Process all PDFs in a folder\n",
    "try:\n",
    "    docs = load_and_chunk_folder(Config.PDF_FOLDER_PATH)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to process folder: {e}\")\n",
    "    raise\n",
    "\n",
    "# Initialize Chroma Vector Store\n",
    "try:\n",
    "    gemini_embeddings = GoogleGenerativeAIEmbeddings(model=Config.EMBEDDING_MODEL)\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=gemini_embeddings,\n",
    "        persist_directory=Config.CHROMA_PERSIST_DIR\n",
    "    )\n",
    "    logger.info(\"Vector store initialized and persisted.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error initializing vector store: {e}\")\n",
    "    raise\n",
    "\n",
    "# Load the persisted vector store from disk\n",
    "try:\n",
    "    vectorstore_disk = Chroma(\n",
    "        persist_directory=Config.CHROMA_PERSIST_DIR,\n",
    "        embedding_function=gemini_embeddings\n",
    "    )\n",
    "    logger.info(\"Loaded vector store from disk.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading vector store from disk: {e}\")\n",
    "    raise\n",
    "\n",
    "# Advanced Indexing: Hierarchical Indexing\n",
    "class HierarchicalIndex:\n",
    "    def __init__(self, documents: List[Document], embedding_model):\n",
    "        self.documents = documents\n",
    "        self.embedding_model = embedding_model\n",
    "        self.index = self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        \"\"\"Build a hierarchical index using embeddings.\"\"\"\n",
    "        embeddings = self.embedding_model.embed_documents([doc.page_content for doc in self.documents])\n",
    "        self.index = self._cluster_documents(embeddings)\n",
    "        return self.index\n",
    "\n",
    "    def _cluster_documents(self, embeddings, n_clusters=10):\n",
    "        \"\"\"Cluster documents into hierarchical groups.\"\"\"\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        clusters = kmeans.fit_predict(embeddings)\n",
    "        return {i: [] for i in range(n_clusters)}\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = Config.TOP_K) -> List[Document]:\n",
    "        \"\"\"Retrieve documents from the hierarchical index.\"\"\"\n",
    "        query_embedding = self.embedding_model.embed_query(query)\n",
    "        # For simplicity, return top-k documents (can be enhanced with hierarchical search)\n",
    "        return self.documents[:top_k]\n",
    "\n",
    "# Contextual Embeddings\n",
    "class ContextualEmbeddings:\n",
    "    def __init__(self, model_name=Config.EMBEDDING_MODEL):\n",
    "        self.model = GoogleGenerativeAIEmbeddings(model=model_name)\n",
    "\n",
    "    def embed(self, text: str) -> List[float]:\n",
    "        return self.model.embed_query(text)\n",
    "\n",
    "    def embed_documents(self, documents: List[str]) -> List[List[float]]:\n",
    "        return self.model.embed_documents(documents)\n",
    "\n",
    "# Hybrid Retrieval System\n",
    "class HybridRetriever:\n",
    "    def __init__(self, vector_store, bm25_retriever):\n",
    "        self.vector_store = vector_store\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = Config.TOP_K) -> List[Document]:\n",
    "        # Semantic Search\n",
    "        vector_results = self.vector_store.similarity_search(query, k=top_k)\n",
    "        # Keyword Search\n",
    "        bm25_docs = self.bm25_retriever.get_relevant_documents(query)\n",
    "        # Combine and deduplicate\n",
    "        combined = vector_results + bm25_docs\n",
    "        seen = set()\n",
    "        return [doc for doc in combined if not (doc.page_content in seen or seen.add(doc.page_content))]\n",
    "\n",
    "# Reranking System\n",
    "class Reranker:\n",
    "    def __init__(self, model_name=Config.RERANKER_MODEL):\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "            logger.info(f\"Loaded reranking model: {model_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading reranking model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def rerank(self, query: str, documents: List[Document], top_n: int = Config.TOP_N) -> List[Document]:\n",
    "        try:\n",
    "            pairs = [(query, doc.page_content) for doc in documents]\n",
    "            features = self.tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            scores = self.model(**features).logits\n",
    "            sorted_indices = scores.argsort(descending=True)\n",
    "            return [documents[i] for i in sorted_indices[:top_n]]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during reranking: {e}\")\n",
    "            return documents[:top_n]  # Fallback to top N documents\n",
    "\n",
    "# Advanced Prompting Strategies\n",
    "class AdvancedPrompts:\n",
    "    @staticmethod\n",
    "    def retrieval_prompt(query: str) -> str:\n",
    "        \"\"\"Advanced prompt for retrieval.\"\"\"\n",
    "        return f\"\"\"\n",
    "        You are an expert retrieval system. Rewrite the following query to improve retrieval:\n",
    "        Original Query: {query}\n",
    "        Rewritten Query:\n",
    "        \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def metadata_filter_prompt(query: str, metadata: Dict[str, str]) -> str:\n",
    "        \"\"\"Advanced prompt for metadata filtering.\"\"\"\n",
    "        return f\"\"\"\n",
    "        You are an expert retrieval system. Add metadata to the query for better filtering:\n",
    "        Original Query: {query}\n",
    "        Metadata: {metadata}\n",
    "        Enhanced Query:\n",
    "        \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def generation_prompt(context: str, question: str) -> str:\n",
    "        \"\"\"Advanced prompt for generation.\"\"\"\n",
    "        return f\"\"\"\n",
    "        You are an expert in talent management and career development. Use the following context to answer the question.\n",
    "        Context: {context}\n",
    "        ---\n",
    "        Question: {question}\n",
    "        Answer in markdown with sources. If unsure, say \"I don't know\".\n",
    "        \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def can_answer_from_history_prompt(query: str, chat_history: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"Prompt to check if the query can be answered from chat history.\"\"\"\n",
    "        return f\"\"\"\n",
    "        Can the following query be answered using the provided chat history? Answer with \"yes\" or \"no\".\n",
    "        Query: {query}\n",
    "        Chat History:\n",
    "        {chat_history}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def enhance_query_with_history_prompt(query: str, chat_history: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"Prompt to enhance the query using chat history.\"\"\"\n",
    "        return f\"\"\"\n",
    "        Rewrite the following query to include relevant context from the chat history only if necessary:\n",
    "        Query: {query}\n",
    "        Chat History:\n",
    "        {chat_history}\n",
    "        Enhanced Query:\n",
    "        \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def answer_from_history_prompt(chat_history: List[Dict[str, str]], question: str) -> str:\n",
    "        \"\"\"Prompt to generate an answer from chat history.\"\"\"\n",
    "        return f\"\"\"\n",
    "        You are an expert in talent management and career development. Use the following chat history to answer the question.\n",
    "        Chat History:\n",
    "        {chat_history}\n",
    "        ---\n",
    "        Question: {question}\n",
    "        Answer in markdown with sources. If unsure, say \"I don't know\".\n",
    "        \"\"\"\n",
    "\n",
    "# Metadata Extraction and Filtering\n",
    "class MetadataExtractor:\n",
    "    @staticmethod\n",
    "    def extract_from_query(query: str) -> Dict[str, str]:\n",
    "        \"\"\"Extract metadata from the query using an LLM.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Extract metadata from the following query. Return the result as a JSON object with keys like \"year\", \"topic\", \"location\", etc.\n",
    "        Query: {query}\n",
    "        Metadata:\n",
    "        \"\"\"\n",
    "        llm = ChatGoogleGenerativeAI(model=Config.GENERATION_MODEL, temperature=0.3)\n",
    "        response = llm.invoke(prompt).content\n",
    "        try:\n",
    "            metadata = json.loads(response)\n",
    "            return metadata\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing metadata: {e}\")\n",
    "            return {}\n",
    "\n",
    "class MetadataInferrer:\n",
    "    @staticmethod\n",
    "    def infer_from_documents(documents: List[Document]) -> Dict[str, str]:\n",
    "        \"\"\"Infer metadata from the document set.\"\"\"\n",
    "        metadata = {}\n",
    "\n",
    "        # Example: Infer the most recent year\n",
    "        years = [doc.metadata.get(\"year\") for doc in documents if \"year\" in doc.metadata]\n",
    "        if years:\n",
    "            metadata[\"year\"] = max(years)\n",
    "\n",
    "        # Example: Infer the most common topic\n",
    "        topics = [doc.metadata.get(\"topic\") for doc in documents if \"topic\" in doc.metadata]\n",
    "        if topics:\n",
    "            metadata[\"topic\"] = Counter(topics).most_common(1)[0][0]\n",
    "\n",
    "        return metadata\n",
    "\n",
    "def combine_metadata(query_metadata: Dict[str, str], document_metadata: Dict[str, str]) -> Dict[str, str]:\n",
    "    \"\"\"Combine metadata from the query and documents.\"\"\"\n",
    "    combined = {}\n",
    "    combined.update(document_metadata)  # Start with document metadata\n",
    "    combined.update(query_metadata)    # Override with query metadata (if any)\n",
    "    return combined\n",
    "\n",
    "# LangGraph Workflow\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve_nodes(state: GraphState) -> GraphState:\n",
    "    try:\n",
    "        # Initialize components\n",
    "        embeddings = ContextualEmbeddings()\n",
    "        hierarchical_index = HierarchicalIndex(docs, embeddings)\n",
    "        bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "        hybrid_retriever = HybridRetriever(vectorstore_disk, bm25_retriever)\n",
    "        reranker = Reranker()\n",
    "\n",
    "        # Extract metadata from the query\n",
    "        query_metadata = MetadataExtractor.extract_from_query(state[\"question\"])\n",
    "\n",
    "        # Infer metadata from the document set\n",
    "        document_metadata = MetadataInferrer.infer_from_documents(docs)\n",
    "\n",
    "        # Combine metadata\n",
    "        metadata = combine_metadata(query_metadata, document_metadata)\n",
    "\n",
    "        # Rewrite query with metadata\n",
    "        llm = ChatGoogleGenerativeAI(model=Config.GENERATION_MODEL, temperature=0.3)\n",
    "        rewritten_query = llm.invoke(AdvancedPrompts.retrieval_prompt(state[\"question\"])).content\n",
    "\n",
    "        # Add metadata filtering\n",
    "        enhanced_query = llm.invoke(AdvancedPrompts.metadata_filter_prompt(rewritten_query, metadata)).content\n",
    "\n",
    "        # Retrieve documents\n",
    "        initial_docs = hybrid_retriever.retrieve(enhanced_query)\n",
    "        state[\"context\"] = reranker.rerank(enhanced_query, initial_docs)\n",
    "        logger.info(f\"Retrieved and reranked {len(state['context'])} documents.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in retrieve_nodes: {e}\")\n",
    "        state[\"context\"] = []  # Fallback to empty context\n",
    "    return state\n",
    "\n",
    "async def generate_answer(state: GraphState):\n",
    "    \"\"\"Stream the answer and store it in state['answer'].\"\"\"\n",
    "    try:\n",
    "        formatted_docs = \"\\n\\n\".join([d.page_content for d in state[\"context\"]])\n",
    "        prompt = AdvancedPrompts.generation_prompt(formatted_docs, state[\"question\"])\n",
    "        llm = ChatGoogleGenerativeAI(model=Config.GENERATION_MODEL, temperature=0.3)\n",
    "        chain = PromptTemplate.from_template(prompt) | llm | StrOutputParser()\n",
    "        response = chain.stream({\n",
    "            \"question\": state[\"question\"],\n",
    "            \"context\": formatted_docs\n",
    "        })\n",
    "\n",
    "        # Stream the answer and build the full response\n",
    "        full_response = \"\"\n",
    "        print(\"Generating answer...\")\n",
    "        for chunk in response:\n",
    "            print(chunk, end=\"\", flush=True)  # Stream to the user\n",
    "            full_response += chunk  # Build the full response\n",
    "        print(\"\\nAnswer generation complete.\")\n",
    "\n",
    "        # Store the full response in state[\"answer\"]\n",
    "        state[\"answer\"] = full_response\n",
    "        return state\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in generate_answer: {e}\")\n",
    "        state[\"answer\"] = \"I'm sorry, I couldn't generate an answer. Please try again.\"\n",
    "        return state\n",
    "\n",
    "def can_answer_from_history(query: str, chat_history: List[Dict[str, str]]) -> bool:\n",
    "    \"\"\"Check if the query can be answered from the chat history.\"\"\"\n",
    "    prompt = AdvancedPrompts.can_answer_from_history_prompt(query, chat_history)\n",
    "    llm = ChatGoogleGenerativeAI(model=Config.GENERATION_MODEL, temperature=0.3)\n",
    "    response = llm.invoke(prompt).content.strip().lower()\n",
    "    return response == \"yes\"\n",
    "\n",
    "def enhance_query_with_history(query: str, chat_history: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"Enhance the query using chat history.\"\"\"\n",
    "    prompt = AdvancedPrompts.enhance_query_with_history_prompt(query, chat_history)\n",
    "    llm = ChatGoogleGenerativeAI(model=Config.GENERATION_MODEL, temperature=0.3)\n",
    "    return llm.invoke(prompt).content\n",
    "\n",
    "# Main Function with Advanced Prompting\n",
    "async def main(userID: str, chatID: str, question: str, top_k: int = Config.TOP_K):\n",
    "    # Validate input\n",
    "    try:\n",
    "        query_input = QueryInput(question=question, top_k=top_k)\n",
    "    except ValidationError as e:\n",
    "        logger.error(f\"Validation error: {e}\")\n",
    "        return\n",
    "\n",
    "    # Retrieve chat history\n",
    "    chat_history = retrieve_chat_history(userID, chatID)\n",
    "    logger.info(f\"Retrieved chat history for user {userID}, chat {chatID}.\")\n",
    "\n",
    "    # Check if the query can be answered from chat history\n",
    "    if can_answer_from_history(query_input.question, chat_history):\n",
    "        logger.info(\"Answering from chat history.\")\n",
    "        # Generate answer from chat history\n",
    "        prompt = AdvancedPrompts.answer_from_history_prompt(chat_history, query_input.question)\n",
    "        llm = ChatGoogleGenerativeAI(model=Config.GENERATION_MODEL, temperature=0.3)\n",
    "        chain = PromptTemplate.from_template(prompt) | llm | StrOutputParser()\n",
    "        response = chain.stream({\n",
    "            \"question\": query_input.question,\n",
    "            \"context\": chat_history\n",
    "        })\n",
    "        for chunk in response:\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "        print(\"\\nAnswer generation complete.\")\n",
    "        return\n",
    "\n",
    "    # Enhance query with chat history\n",
    "    enhanced_query = enhance_query_with_history(query_input.question, chat_history)\n",
    "    logger.info(f\"Enhanced query: {enhanced_query}\")\n",
    "\n",
    "    # Initialize workflow\n",
    "    workflow = StateGraph(GraphState)\n",
    "    workflow.add_node(\"retrieve\", retrieve_nodes)\n",
    "    workflow.set_entry_point(\"retrieve\")\n",
    "    workflow.add_edge(\"retrieve\", END)\n",
    "    app = workflow.compile()\n",
    "\n",
    "    # Run retrieval\n",
    "    state = app.invoke({\"question\": enhanced_query})\n",
    "\n",
    "    # Generate the answer (stream and store in state[\"answer\"])\n",
    "    state = await generate_answer(state)\n",
    "\n",
    "    # Store the question and answer in chat history\n",
    "    store_chat_history(userID, chatID, \"user\", query_input.question)\n",
    "    store_chat_history(userID, chatID, \"assistant\", state[\"answer\"])\n",
    "    logger.info(\"Stored chat history.\")\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    userID = \"user123\"\n",
    "    chatID = \"chat456\"\n",
    "    question = \"Run a SWOT analysis and tell me if this is good for me to leave my job to sign up as a professional with the new company\"\n",
    "\n",
    "    # Check if an event loop is already running\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "\n",
    "    # Run the main function\n",
    "    loop.run_until_complete(main(userID, chatID, question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we are sure our pipeline will work let's convert it into a full blown application prototype. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further upgrade\n",
    "- Optimize for speed\n",
    "- Finetune a custom model\n",
    "- Explore other strategies to develop your system\n",
    "- Develop a full web and mobile App and optimize the UI UX and AI functionality for target audience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa_chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
